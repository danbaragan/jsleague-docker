<!-- SUBTITLE: Dockerfile -->

<Layout header>

# Dockerfile

</Layout>

---

<Layout>

<CodeSurferColumns>

<Step>

```makefile
FROM alpine
```

<div>

Any dockerfile should start with a point of origin.

In this case, we're using a bare-bones alpine linux distribution.

Other options are, for example: <Code>ubuntu</Code>, <Code>node</Code>, <Code>python</Code>, etc...

</div>

</Step>

<Step>

```makefile 3
FROM alpine

RUN app add node
```

<div>

One of the most used commands is the `run` command. 

It will simply execute a command on the container, at build-time.

Basically, anything that you *can* use in the container (basic linux tools, such as: `ls`, `pwd`, `cd`, etc and subsequent installed tools) will function here.

</div>

</Step>

<Step>

```makefile 5:6,3[13:17]
FROM alpine

RUN app add node

COPY package.json .
COPY package-lock.json .
```

<div>

The second-most used command in Dockerfiles is the `copy` command;

Its purpose is to copy files from the host operating system to the container, to be used in future build steps.

In this case, we're copying the dependencies files for a nodejs application.

</div>

</Step>

<Step>

```makefile 8,3[13:17]
FROM alpine

RUN app add node

COPY package.json .
COPY package-lock.json .

RUN npm install
```

<div>

Since we have a list of dependencies, we can now install them.

</div>

</Step>

<Step>

```makefile 10:11,3[13:17]
FROM alpine

RUN app add node

COPY package.json .
COPY package-lock.json .

RUN npm install

ENTRYPOINT echo "NODE: $(node -v) & NPM: $(npm -v)"
CMD npm run start
```

<div>

To actually run something in the container, we have a few options.

One is the `entrypoint` command. It defines how the container should execute if ran by itself.

Another is the `cmd` command. It is similar to the `entrypoint` command, but it can be overriden while running the container.

</div>

</Step>

<Step>

```makefile 10:11
FROM alpine

RUN app add node

COPY package.json .
COPY package-lock.json .

RUN npm install

ENTRYPOINT echo "NODE: $(node -v) & NPM: $(npm -v)"
CMD npm run start
```

<div>

Let's take a closer look at `entrypoint` and `cmd`.

`Entrypoint` is an instruction always executed at a `docker run` command, whereas

`Cmd` is an instruction that can be overriden at run time. 

</div>

</Step>

<Step>

```makefile 1[6:11],5:6
FROM mongo

[...]

ENTRYPOINT mongod &
CMD mongo
```

<div>

For instance, if we have a *MongoDB* instance, which, naturally, supplies a server (`mongod`) and a client to manipulate and inspect data (`mongo`), we'd want the server to be always on.

Therefore, the `entrypoint` must be `mongod`, and the command `mongo`. If, for instance, I need to run bash, or some import script on the container at runtime, I can make sure that the mongo server is online, regardless of run command.

</div>

</Step>

<Step>

```makefile 1
FROM nginx

COPY index.html /www
RUN mkdir -p /www/pages
VOLUME /pages
```

<div>

Let's talk files and directories. File communication can happen in two ways in Docker. 

Let's assume we want the following structure for a simple HTTP server (let's use nginx for this):

```
/
  - index.html
  - www/
    - page1.html
    - page2.html
    ...
```

</div>

</Step>

<Step>

```makefile 3
FROM nginx

COPY index.html /www
RUN mkdir -p /www/pages
VOLUME /pages
```

<div>

One of the methods used for file transfer would be the `copy` instruction, as pointed out earlier.

</div>

</Step>

<Step>

```makefile 4:5
FROM nginx

COPY index.html /www
RUN mkdir -p /www/pages
VOLUME /pages
```

<div>

However, for a number of reason, copying files and re-building the image is a time-consuming, and, ultimately, needless procedure.

Therefore, volumes were introduced. Volumes are simple mounts, that act, in simplistic terms, like symlinks (in Windows, Unix, etc).

</div>

</Step>

<Step>

```makefile 5
FROM nginx

COPY index.html /www
RUN mkdir -p /www/pages
VOLUME /pages
```

<div>

At runtime, the volume paths (on host and guest, both) must be specified, as well as, optionally, the mount type (read only, read write, etc).

For instance, the current folder (containing HTML files) can be linked to the /www/pages folder in the container, as follows: 

```
$ docker run -v `pwd`:/www/pages ...
```

</div>

</Step>

<Step>

```makefile 5
FROM nginx

COPY index.html /www
RUN mkdir -p /www/pages
VOLUME /pages
```

<div>

In this case, any file modified on the host directory, will have its changes reflected on the container. 

Just as well, any changes occurring on the container will be reflected on the host.

This is useful in 2 cases: Either during development, so that rebuilding is unnecessary, either in production, with shared data between containers and hosts (the data of a database, for instace, so that it persists between container starting and stopping)

</div>

</Step>

<Step>

```makefile
...
```

<div>

Next, we need to talk about making Dockerfiles more dynamic. For instance, by providing arguments at build-time, or providing environment variables at run-time.

</div>

</Step>

<Step>

```makefile
...
```

<div>

![ARG and ENV](https://d33wubrfki0l68.cloudfront.net/8d799fc311d166c3a7d9f1a7970e1dd1d43bf141/0a855/images/docker-env-vars/docker_environment_build_args.png)

</div>

</Step>

<Step>

```makefile 1
ARG NODE_VER=10

FROM node:${NODE_VER}

ARG NODE_VER

ENV ENV_VER=$NODE_VER

RUN echo $NODE_VER
RUN echo $ENV_VER

CMD env
```

<div>

The `ARG` instruction will make a variable available inside the container, which can receive a default value, and can be configured at build-time (by using the `--build-arg` argument).

It is also the **only** instruction that can be used **before** `FROM`. 

</div>

</Step>

<Step>

```makefile 3,5
ARG NODE_VER=10

FROM node:${NODE_VER}

ARG NODE_VER

ENV ENV_VER=$NODE_VER

RUN echo $NODE_VER
RUN echo $ENV_VER

CMD env
```

<div>

We can then use the argument in our `FROM` instruction, and, since the initial `ARG` instruction was before the first `FROM`, we need to bring in in, again.

Why? Think in term of intermediate steps. Before the `FROM` command, nothing really exists.

</div>

</Step>

<Step>

```makefile 7
ARG NODE_VER=10

FROM node:${NODE_VER}

ARG NODE_VER

ENV ENV_VER=$NODE_VER

RUN echo $NODE_VER
RUN echo $ENV_VER

CMD env
```

<div>

To define an environment variable, we use `ENV`, which, like the `ARG` instruction takes in an optional default value.

In this case, why not set it as the `NODE_VER` argument?

</div>

</Step>

<Step>

```makefile 9:10
ARG NODE_VER=10

FROM node:${NODE_VER}

ARG NODE_VER

ENV ENV_VER=$NODE_VER

RUN echo $NODE_VER
RUN echo $ENV_VER

CMD env
```

<div>

While running this, you will notice that both of these will return the same value.

</div>

</Step>

<Step>

```makefile 12
ARG NODE_VER=10

FROM node:${NODE_VER}

ARG NODE_VER

ENV ENV_VER=$NODE_VER

RUN echo $NODE_VER
RUN echo $ENV_VER

CMD env
```

<div>

If we run `env` as a default command on the image, you will notice that, upon running it, the default `ENV_VER` will be the same as the one we used while building. 

Just as well, we can over-write the value at run-time by using the `-e` argument for `docker run`: 

```
$ docker run -e ENV_VER=25
```

</div>

</Step>

<Step>

```makefile 12
ARG NODE_VER=10

FROM node:${NODE_VER}

ARG NODE_VER

ENV ENV_VER=$NODE_VER

RUN echo $NODE_VER
RUN echo $ENV_VER

CMD env
```

<div>

Similarly, a `.env` (dot-env) file can be used to supply all arguments in one go

```
$ docker run --env-file .env
```

</div>

</Step>

<Step>

```makefile
FROM node

[...]

EXPOSE 80
CMD node server.js -p 80
```

<div>

The last thing we should talk about today is data transfer in the form of information, rather than files, at run-time.

This can be achieved through exporting and mapping ports. For instance, we can have a `nodejs` application running on port `80` within the container, which we will map to port `8000` on the host, thus making the app accessible at port 8000.

</div>

</Step>

<Step>

```makefile 5
FROM node

[...]

EXPOSE 80
CMD node server.js -p 80
```

<div>

First, we expose the port. Of course, this would be a great place to use args, but we'll leave that for tomorrow.

Then, after the image is built, we can bind the ports while running the container by using the `-p` argument:

```
$ docker run -p 80:8000 
```

</div>

</Step>

</CodeSurferColumns>

</Layout>